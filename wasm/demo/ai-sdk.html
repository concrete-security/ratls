<!DOCTYPE html>
<html>
<head>
  <title>RA-TLS + AI SDK Demo</title>
  <style>
    * { box-sizing: border-box; }
    body {
      font-family: 'JetBrains Mono', 'SF Mono', 'Fira Code', monospace;
      background: linear-gradient(135deg, #0f0f1a 0%, #1a1a2e 50%, #16213e 100%);
      color: #e0e0e0;
      min-height: 100vh;
      margin: 0;
      padding: 40px 20px;
    }
    .container {
      max-width: 900px;
      margin: 0 auto;
    }
    h1 {
      color: #00d4aa;
      font-size: 1.8rem;
      margin-bottom: 8px;
      text-shadow: 0 0 20px rgba(0, 212, 170, 0.3);
    }
    .subtitle {
      color: #888;
      margin-bottom: 32px;
      font-size: 0.9rem;
    }
    .card {
      background: rgba(255, 255, 255, 0.03);
      border: 1px solid rgba(255, 255, 255, 0.08);
      border-radius: 12px;
      padding: 24px;
      margin-bottom: 20px;
      backdrop-filter: blur(10px);
    }
    .card h2 {
      color: #00d4aa;
      font-size: 1rem;
      margin: 0 0 16px 0;
      display: flex;
      align-items: center;
      gap: 8px;
    }
    .badge {
      background: rgba(0, 212, 170, 0.15);
      color: #00d4aa;
      padding: 2px 8px;
      border-radius: 4px;
      font-size: 0.75rem;
    }
    .badge.warning {
      background: rgba(255, 170, 0, 0.15);
      color: #ffaa00;
    }
    .input-group {
      margin-bottom: 16px;
    }
    label {
      display: block;
      color: #888;
      font-size: 0.8rem;
      margin-bottom: 6px;
    }
    input, textarea {
      width: 100%;
      background: rgba(0, 0, 0, 0.3);
      border: 1px solid rgba(255, 255, 255, 0.1);
      border-radius: 6px;
      padding: 12px;
      color: #e0e0e0;
      font-family: inherit;
      font-size: 0.9rem;
    }
    input:focus, textarea:focus {
      outline: none;
      border-color: #00d4aa;
      box-shadow: 0 0 0 2px rgba(0, 212, 170, 0.2);
    }
    textarea {
      min-height: 80px;
      resize: vertical;
    }
    button {
      background: linear-gradient(135deg, #00d4aa 0%, #00a88a 100%);
      color: #0f0f1a;
      border: none;
      padding: 12px 24px;
      border-radius: 6px;
      font-family: inherit;
      font-weight: 600;
      cursor: pointer;
      transition: transform 0.1s, box-shadow 0.2s;
    }
    button:hover {
      transform: translateY(-1px);
      box-shadow: 0 4px 20px rgba(0, 212, 170, 0.3);
    }
    button:disabled {
      opacity: 0.5;
      cursor: not-allowed;
      transform: none;
    }
    #output {
      background: rgba(0, 0, 0, 0.4);
      border-radius: 8px;
      padding: 20px;
      font-size: 0.85rem;
      line-height: 1.6;
      white-space: pre-wrap;
      word-break: break-word;
      min-height: 200px;
      max-height: 500px;
      overflow-y: auto;
    }
    .log-info { color: #888; }
    .log-success { color: #00d4aa; }
    .log-error { color: #ff6b6b; }
    .log-stream { color: #ffd93d; }
    .attestation {
      background: rgba(0, 212, 170, 0.1);
      border: 1px solid rgba(0, 212, 170, 0.3);
      border-radius: 6px;
      padding: 12px;
      margin: 12px 0;
      font-size: 0.8rem;
    }
    .spinner {
      display: inline-block;
      width: 14px;
      height: 14px;
      border: 2px solid rgba(0, 212, 170, 0.3);
      border-top-color: #00d4aa;
      border-radius: 50%;
      animation: spin 0.8s linear infinite;
      margin-right: 8px;
    }
    @keyframes spin {
      to { transform: rotate(360deg); }
    }
  </style>
</head>
<body>
  <div class="container">
    <h1>üîê RA-TLS + AI SDK Demo</h1>
    <p class="subtitle">Using Vercel AI SDK with attested TLS connections to TEE-hosted LLMs</p>

    <div class="card">
      <h2>Configuration</h2>
      <div class="input-group">
        <label>Proxy URL</label>
        <input type="text" id="proxyUrl" value="ws://127.0.0.1:9000">
      </div>
      <div class="input-group">
        <label>Target Host</label>
        <input type="text" id="targetHost" value="vllm.concrete-security.com">
      </div>
      <div class="input-group">
        <label>Model</label>
        <input type="text" id="model" value="openai/gpt-oss-120b">
      </div>
    </div>

    <div class="card">
      <h2>Chat <span class="badge">streaming</span></h2>
      <div class="input-group">
        <label>Message</label>
        <textarea id="message">Explain quantum computing in 2 sentences.</textarea>
      </div>
      <button id="sendBtn" onclick="sendMessage()">Send with AI SDK</button>
    </div>

    <div class="card">
      <h2>Output</h2>
      <div id="output"><span class="log-info">Ready. Click "Send with AI SDK" to test the integration.</span></div>
    </div>
  </div>

  <script type="importmap">
  {
    "imports": {
      "ai": "https://esm.sh/ai@4.3.16",
      "@ai-sdk/openai": "https://esm.sh/@ai-sdk/openai@1.3.22"
    }
  }
  </script>

  <script type="module">
    import { createOpenAI } from "@ai-sdk/openai";
    import { generateText, streamText } from "ai";
    import { init, createRatlsFetch, mergeWithDefaultAppCompose } from "../pkg/ratls-fetch.js";

    const output = document.getElementById("output");
    const sendBtn = document.getElementById("sendBtn");

    // Docker compose content for vllm.concrete-security.com
    const VLLM_DOCKER_COMPOSE = `services:
  vllm:
    image: vllm/vllm-openai:v0.13.0
    container_name: vllm
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
    volumes:
      - huggingface-cache:/root/.cache/huggingface
    runtime: nvidia
    # No external ports - internal access only
    expose:
      - "8000"
    networks:
      - vllm
    command: >
      --model openai/gpt-oss-120b
      --tensor-parallel-size 1
      --gpu-memory-utilization 0.95
      --max-model-len 131072
      --max-num-seqs 8
      --reasoning-parser openai_gptoss
      --enable-auto-tool-choice
      --tool-call-parser openai
      --async-scheduling
      --allowed-origins []
    restart: unless-stopped
    healthcheck:
      start_interval: 1h30m # Allow up to 90 minutes for initial model loading
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Combined Nginx reverse proxy and Certificate Manager
  nginx-cert-manager:
    image: ghcr.io/concrete-security/cert-manager@sha256:92655a24060497516ea0cfd79b7fbfb599f13d303eb0c3e9c79cf8c5ee9cc1d1
    container_name: nginx-cert-manager
    ports:
      - "80:80"
      - "443:443"
    environment:
      - DOMAIN=vllm.concrete-security.com
      - DEV_MODE=false
      - LETSENCRYPT_STAGING=false
      # Used to versionize accounts: can change account by using another env variable
      # Useful in case an account reaches a rate limit
      - LETSENCRYPT_ACCOUNT_VERSION=v1
      # Force removal of existing certificate files on startup
      - FORCE_RM_CERT_FILES=false
      # Set log level
      - LOG_LEVEL=INFO
    volumes:
      - tls-certs-keys:/etc/nginx/ssl/
      - /var/run/dstack.sock:/var/run/dstack.sock
    networks:
      - vllm
      - attestation
      - auth
    restart: unless-stopped

  # Auth service for protected endpoints
  auth-service:
    image: ghcr.io/concrete-security/auth-service@sha256:f819c57d1648a4b4340fc296ef9872e43b70c7190d67a93820cf4f7b657d5310
    container_name: auth-service
    environment:
      - HOST=0.0.0.0
      - PORT=8081
      - AUTH_SERVICE_TOKEN=\${AUTH_SERVICE_TOKEN}
      - LOG_LEVEL=INFO
    expose:
      - "8081"
    networks:
      - auth
    restart: unless-stopped

  # TDX Attestation service
  attestation-service:
    image: ghcr.io/concrete-security/attestation-service@sha256:ad98abfe2d97fd2f25beba4a7e343376bce2ac0e8c3ed2ded97b38b06df12841
    container_name: attestation-service
    environment:
      - HOST=0.0.0.0
      - PORT=8080
      # Keep it to 1 if you want replication at the container orchestration level (see deploy.replicas)
      - WORKERS=1
    volumes:
      - /var/run/dstack.sock:/var/run/dstack.sock
    expose:
      - "8080"
    networks:
      - attestation
    restart: unless-stopped
    deploy:
      mode: replicated
      # Keep it to 1 if you want replication at the process level (see env.WORKERS)
      replicas: 1

networks:
  vllm:
    driver: bridge
  attestation:
    driver: bridge
  auth:
    driver: bridge

volumes:
  # Used to store huggingface models
  huggingface-cache:
  # TLS certificates and keys
  tls-certs-keys:
`;

    function log(msg, type = "info") {
      console.log(`[${type}]`, msg);
      const span = document.createElement("span");
      span.className = `log-${type}`;
      span.textContent = msg + "\n";
      output.appendChild(span);
      output.scrollTop = output.scrollHeight;
    }

    function clear() {
      output.innerHTML = "";
    }

    let attestationData = null;

    window.sendMessage = async function() {
      clear();
      sendBtn.disabled = true;
      sendBtn.innerHTML = '<span class="spinner"></span>Connecting...';

      try {
        const proxyUrl = document.getElementById("proxyUrl").value;
        const targetHost = document.getElementById("targetHost").value;
        const model = document.getElementById("model").value;
        const message = document.getElementById("message").value;

        log("Initializing WASM...", "info");
        await init();
        log("WASM ready.\n", "success");

        // Full production policy for vllm.concrete-security.com
        // Must be created after init() since mergeWithDefaultAppCompose requires WASM
        const VLLM_POLICY = {
          type: "dstack_tdx",
          expected_bootchain: {
            mrtd: "b24d3b24e9e3c16012376b52362ca09856c4adecb709d5fac33addf1c47e193da075b125b6c364115771390a5461e217",
            rtmr0: "24c15e08c07aa01c531cbd7e8ba28f8cb62e78f6171bf6a8e0800714a65dd5efd3a06bf0cf5433c02bbfac839434b418",
            rtmr1: "6e1afb7464ed0b941e8f5bf5b725cf1df9425e8105e3348dca52502f27c453f3018a28b90749cf05199d5a17820101a7",
            rtmr2: "89e73cedf48f976ffebe8ac1129790ff59a0f52d54d969cb73455b1a79793f1dc16edc3b1fccc0fd65ea5905774bbd57"
          },
          os_image_hash: "86b181377635db21c415f9ece8cc8505f7d4936ad3be7043969005a8c4690c1a",
          app_compose: mergeWithDefaultAppCompose({
            docker_compose_file: VLLM_DOCKER_COMPOSE,
            allowed_envs: ["AUTH_SERVICE_TOKEN"]
          }),
          allowed_tcb_status: ["UpToDate", "SWHardeningNeeded"]
        };

        log(`Creating RA-TLS fetch for ${targetHost} with full verification...`, "info");

        const ratlsFetch = createRatlsFetch({
          proxyUrl,
          targetHost,
          policy: VLLM_POLICY,
          onAttestation: (att) => {
            attestationData = att;
            log("\n‚úì TEE Attestation verified!", "success");
            const attDiv = document.createElement("div");
            attDiv.className = "attestation";
            attDiv.innerHTML = `<strong>Attestation:</strong>\n${JSON.stringify(att, null, 2)}`;
            output.appendChild(attDiv);
          }
        });

        log("Creating OpenAI provider with RA-TLS fetch...", "info");

        const openai = createOpenAI({
          baseURL: `https://${targetHost}/v1`,
          apiKey: "not-needed",
          fetch: ratlsFetch
        });

        sendBtn.innerHTML = '<span class="spinner"></span>Streaming...';

        log(`\nSending message: "${message}"`, "info");
        log(`Model: ${model}\n`, "info");
        log("--- Streaming Response ---", "stream");

        const { textStream } = await streamText({
          model: openai(model),
          messages: [{ role: "user", content: message }],
          maxTokens: 200
        });

        for await (const chunk of textStream) {
          const span = document.createElement("span");
          span.className = "log-stream";
          span.textContent = chunk;
          output.appendChild(span);
          output.scrollTop = output.scrollHeight;
        }

        log("\n\n--- Stream Complete ---", "success");

      } catch (error) {
        log(`\nError: ${error?.message || String(error)}`, "error");
        console.error(error);
      } finally {
        sendBtn.disabled = false;
        sendBtn.innerHTML = "Send with AI SDK";
      }
    };
  </script>
</body>
</html>

